{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhSHaethaRr8+ht8gPN9F6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatc2/Machine-Learning/blob/main/Tokenizer/toeknizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "wZsPoSGdt6ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prepare Data"
      ],
      "metadata": {
        "id": "KdRcVityuzmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install sentencepiece\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BcGGDcGuyZf",
        "outputId": "1861bc0b-d2ff-4d4f-d88a-c69ff8869c02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/drive/My Drive/CS441/hw3/\"\n",
        "\n",
        "%cd /content/drive/My Drive/CS441/hw3/\n",
        "!mkdir wikitext-2\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
        "!unzip wikitext-2-v1.zip -d wikitext-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOCsanhqu3YP",
        "outputId": "4560d2ae-4363-47cd-c1fa-71cfa0129595"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/CS441/hw3\n",
            "mkdir: cannot create directory ‘wikitext-2’: File exists\n",
            "--2024-09-25 19:51:00--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.36.190, 52.216.26.182, 52.217.236.0, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.36.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2024-09-25 19:51:00 ERROR 403: Forbidden.\n",
            "\n",
            "unzip:  cannot find or open wikitext-2-v1.zip, wikitext-2-v1.zip.zip or wikitext-2-v1.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML; import sentencepiece as spm; from tokenizers.trainers import BpeTrainer, WordPieceTrainer; from tokenizers import Tokenizer\n",
        "import tokenizers; from tokenizers.pre_tokenizers import Whitespace; from tokenizers import Tokenizer; from tokenizers.models import BPE, WordPiece\n",
        "\n",
        "# Load WikiText-2\n",
        "\n",
        "fil_dir=datadir+\"wikitext-2/wiki.train.tokens\"\n",
        "with open(fil_dir, \"r\") as f:\n",
        "    text = f.read()\n",
        "first_sentence= \"I am learning about word tokenizers. They are not very complicated, and they are a good way to convert natural text into tokens.\"\n",
        "second_sentence= \"Word tokenizers are widely used in natural language processing tasks such as text classification, sentiment analysis, and machine translation.\"\n"
      ],
      "metadata": {
        "id": "yDsvGhB9vXZb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Wordpiece Tokenizer"
      ],
      "metadata": {
        "id": "Bbe7lkRlvThU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WordPiece Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "trainer =WordPieceTrainer(\n",
        "    vocab_size=8000,\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "    min_frequency=2,\n",
        "    show_progress=True\n",
        ")\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train_from_iterator([text], trainer=trainer)\n",
        "\n",
        "# Encode the sentence using WordPiece\n",
        "encoded_sentence = tokenizer.encode(first_sentence)\n",
        "\n",
        "# Decode the encoded sentence using WordPiece\n",
        "decoded_sentence = tokenizer.decode(encoded_sentence.ids)\n",
        "\n",
        "print(encoded_sentence.tokens)\n",
        "print(encoded_sentence.ids)\n",
        "\n",
        "\n",
        "# Encode the sentence using WordPiece\n",
        "encoded_sentence = tokenizer.encode(second_sentence)\n",
        "\n",
        "# Decode the encoded sentence using WordPiece\n",
        "decoded_sentence = tokenizer.decode(encoded_sentence.ids)\n",
        "\n",
        "print(encoded_sentence.tokens)\n",
        "print(encoded_sentence.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RsUL0wnu799",
        "outputId": "25934260-09cc-487c-8ed0-b53744eed98b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'learning', 'about', 'word', 'to', '##ken', '##ize', '##r', '##s', '.', 'They', 'are', 'not', 'very', 'compl', '##icated', ',', 'and', 'they', 'are', 'a', 'good', 'way', 'to', 'conver', '##t', 'natural', 'text', 'into', 'to', '##ken', '##s', '.']\n",
            "[45, 963, 7340, 867, 3300, 474, 3824, 1931, 288, 302, 18, 1227, 595, 606, 1509, 3254, 2497, 16, 470, 727, 595, 68, 2042, 1538, 474, 5027, 289, 3554, 4063, 771, 474, 3824, 302, 18]\n",
            "['W', '##ord', 'to', '##ken', '##ize', '##r', '##s', 'are', 'widely', 'used', 'in', 'natural', 'language', 'process', '##ing', 'task', '##s', 'such', 'as', 'text', 'class', '##ification', ',', 'sent', '##iment', 'analysis', ',', 'and', 'machine', 'transl', '##ation', '.']\n",
            "[59, 614, 474, 3824, 1931, 288, 302, 595, 4050, 951, 465, 3554, 3027, 2303, 471, 4290, 302, 943, 504, 4063, 1418, 3493, 16, 1592, 1777, 5076, 16, 470, 3790, 3935, 518, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fil_dir=datadir+\"wikitext-2/abcd.txt\"\n",
        "print(fil_dir)\n",
        "with open(fil_dir, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "#print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmRE6LrJvkGz",
        "outputId": "d3dbed75-b962-4a01-87a4-39cd55b17717"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/CS441/hw3/wikitext-2/abcd.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Sentence Piece Tokenizer"
      ],
      "metadata": {
        "id": "wjGdaZ5Kvo-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(input=fil_dir, model_prefix=\"wikitext2\", vocab_size=400)\n",
        "\n",
        "# Load tokenizer model\n",
        "tokenizer = spm.SentencePieceProcessor(model_file=\"wikitext2.model\")\n",
        "\n",
        "# Example usage\n",
        "tokens = tokenizer.encode_as_pieces(first_sentence)\n",
        "print(tokens)\n",
        "\n",
        "# Example usage\n",
        "tokens = tokenizer.encode_as_pieces(second_sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpHYqsGFvxHK",
        "outputId": "8c702b03-e168-4a9d-e427-a3b93ce9ea67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁I', '▁', 'am', '▁le', 'ar', 'n', 'ing', '▁a', 'b', 'ou', 't', '▁', 'wor', 'd', '▁to', 'ke', 'n', 'i', 'z', 'er', 's', '.', '▁T', 'he', 'y', '▁a', 're', '▁', 'no', 't', '▁', 've', 'ry', '▁c', 'om', 'p', 'l', 'ic', 'ate', 'd', ',', '▁', 'and', '▁', 'the', 'y', '▁a', 're', '▁a', '▁', 'g', 'o', 'o', 'd', '▁wa', 'y', '▁to', '▁con', 've', 'r', 't', '▁', 'n', 'at', 'ur', 'al', '▁t', 'e', 'x', 't', '▁in', 'to', '▁to', 'ke', 'n', 's', '.']\n",
            "['▁W', 'or', 'd', '▁to', 'ke', 'n', 'i', 'z', 'er', 's', '▁a', 're', '▁w', 'i', 'de', 'l', 'y', '▁', 'u', 's', 'ed', '▁in', '▁', 'n', 'at', 'ur', 'al', '▁', 'la', 'ngu', 'age', '▁pro', 'ces', 's', 'ing', '▁t', 'as', 'k', 's', '▁suc', 'h', '▁as', '▁t', 'e', 'x', 't', '▁c', 'la', 's', 's', 'i', 'f', 'ic', 'ation', ',', '▁se', 'n', 'ti', 'ment', '▁an', 'al', 'y', 's', 'is', ',', '▁', 'and', '▁ma', 'ch', 'in', 'e', '▁', 'tra', 'n', 's', 'l', 'ation', '.']\n"
          ]
        }
      ]
    }
  ]
}